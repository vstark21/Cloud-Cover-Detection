---
SEED: 2022
VERBOSE: 1
DEBUG: False

NAME: 'CCD'

MODEL: 'Unet' # 'CloudNetp', 'Unet', 'SwinSegmenter', 'SeMask' or 'SegNet'
NUM_WORKERS: 2
MODEL_PARAMS:
    CloudNetp:
        meta_enc_cfg:
            type: 'MetaEncoder'
            input_size: 93
        n_channels: 5
        n_classes: 1
        inception_depth: 6
        residual: True
    SwinSegmenter:
        cfg:
            backbone:
                type: 'SwinTransformer'
                img_size: 512
                in_chans: 4
                embed_dim: 96
                depths: [2, 2, 18, 2]
                num_heads: [3, 6, 12, 24]
                window_size: 7
                mlp_ratio: 4.
                qkv_bias: True
                qk_scale: null
                drop_rate: 0.
                attn_drop_rate: 0.
                drop_path_rate: 0.3
                ape: False
                patch_norm: True
                out_indices: [0, 1, 2, 3]
                use_checkpoint: False
            decode_head:
                type: 'UPerHead'
                in_channels: [96, 192, 384, 768]
                in_index: [0, 1, 2, 3]
                pool_scales: [1, 2, 3, 6]
                channels: 512
                dropout_ratio: 0.1
                num_classes: 512
                norm_cfg: 
                    type: 'BN2d'
                    requires_grad: True
                align_corners: False
            auxiliary_head:
                type: 'FCNHead'
                in_channels: 384
                in_index: 2
                channels: 256
                num_convs: 1
                concat_input: False
                dropout_ratio: 0.1
                num_classes: 512
                norm_cfg:  
                    type: 'BN2d'
                    requires_grad: True
                align_corners: False
    SeMask:
        cfg:
            backbone:
                type: 'SeMaskSwinTransformer'
                img_size: 512
                in_chans: 4
                embed_dim: 96
                depths: [2, 2, 18, 2]
                num_heads: [3, 6, 12, 24]
                window_size: 7
                num_cls: 512
                sem_window_size: 7
                num_sem_blocks: [1, 1, 1, 1]
                mlp_ratio: 4.
                qkv_bias: True
                qk_scale: null
                drop_rate: 0.
                attn_drop_rate: 0.
                drop_path_rate: 0.3
                ape: False
                patch_norm: True
                out_indices: [0, 1, 2, 3]
                use_checkpoint: False
            decode_head:
                type: 'BranchFPNHead'
                in_channels: [96, 192, 384, 768]
                in_index: [0, 1, 2, 3]
                feature_strides: [4, 8, 16, 32]
                channels: 256
                dropout_ratio: 0.1
                num_classes: 512
                norm_cfg: 
                    type: 'BN2d'
                    requires_grad: True
                align_corners: False
    SegNet:
        in_channels: 4
        out_channels: 1
        bn_momentum: 0.5
    Unet:
        cfg:
            meta_encoder:
                type: 'MetaEncoder'
                input_size: 93
            backbone:
                type: 'ResNet'
                variant: 'resnet50'
                n_channels: 5
            head:
                type: 'UnetDecodeHead'
                n_classes: 1
                dropout: 0.2
                out_channels: [256, 128, 64, 32, 16]

# Data
DATA_PATH: "data/"
BAD_CHIPS_FILE: "bad_chips.json"
META_DATA_FILE: "train_metadata.csv"
DATA_MEAN: [2848.06411202, 2839.08714853, 2741.28910764, 3657.90921129]
DATA_STD: [3156.92684648, 2899.28014451, 2789.96160889, 2424.18942846]
LOCATIONS: ["Addis Ababa", "Adelaide", "Adi Ramets", "Alexander Bay", "Alto Hama", "Angkor Wat", "Angola", "Asmara nort", "Australia - Central", "Australia - Central West", "Australia - North West", "Bahir Dar", "Bambesa", "Bechar", "Beira", "Bor", "Bunbury", "Cabo Verdo", "Canary Islands", "Chibemba", "Chifunfu", "Chingola", "Cordoba", "DRC", "Ecuador", "Eritrea", "Eswatini", "Ethiopia", "Gabon", "Georgetown", "Ghana", "Harare", "Isiro", "Jimma", "Juba", "Kalamie", "Kenya", "Kimberley", "Kolwezi", "Launceston", "Lodwar", "Lusaka", "Macapa", "Madagascar", "Mahajanga", "Malabo", "Malawi", "Malemba Nkulu", "Manaus", "Matadi", "Monongue", "Morocco", "Nairobi", "Paijan", "Para", "Paraguay", "Pibor", "Port Augusta", "Port Gentil", "Porto Velho", "Rio Branco", "Riobamba", "Rustenburg", "San Lorenzo", "San Sabastian de La Gomera", "Santa Cruz de la Sierra", "Santa Cruz do Sul", "Santa Fe", "Senanga", "Sfax", "Sierra Leone", "South Africa", "South America - Argentina", "South America - Brazil", "South America - Peru", "South America - Suriname", "Sudan", "Timbuktu", "Tunis", "Uganda", "Vitoria"]

# Loss
LOSS_CFG:
    losses:
        bce_loss: "BceLoss"
        dice_loss: "DiceLoss"
        jacc_loss: "JaccardLoss"
        fjacc_loss: "FilteredJaccardLoss"
    loss_weights: 
        bce_loss: 0.
        dice_loss: 0.
        jacc_loss: 1.
        fjacc_loss: 0.
    out_weights:
        out: 1.
        aux_out: 0.4

# Training
OPTIMIZER_CFG: 
    type: 'AdamW' # Should be one of the algorithms in https://pytorch.org/docs/stable/optim.html
    lr: 5.0e-4
    betas: [0.9, 0.999]
    weight_decay: 0.01
TRAIN_BATCH_SIZE: 4
EPOCHS: 128
AMP: True
TRAIN_ITERS: 1024
N_ACCUMULATE: 16
MIN_LEARNING_RATE: 1.0e-5
SCHEDULER: "ReduceLROnPlateau" # Should be one of the scheduler in https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
SCHEDULER_PARAMS:
    mode: "min"
    patience: 2
    factor: 0.5

# Validating
VAL_BATCH_SIZE: 4

# Wandb 
USE_WANDB: True

# Outputs
OUTPUT_PATH: 'outputs/'
LOG_FILE: 'outputs/logs.log'
